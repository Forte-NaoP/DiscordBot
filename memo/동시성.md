
- `std::process::Stdio::piped()` 는 새로운 파이프를 생성한다. `std::process::Command::stdin(Stdio::piped())` 같은 식으로 사용하면 stdin이 생성된 파이프로 대체 되는 식.<br>
`CommandObject.stdin.take().unwrap()`으로 해당 파이프 디스크립터를 가져올 수 있다.

- `Command::spawn()`으로 생성된 `Child` 객체는 `wait[_with_output]`나 `kill` 호출 전까지 완료/종료되지 않는다.<br>
만약 파일을 저장하는 작업 이후 연속해서 다른 `Command` 가 그 파일을 읽으려 한다면 파일이 생성되지 않아 정상적으로 작동하지 않는다. 버퍼 관련 문제인듯<br>
따라서 `wait` 를 사용해 프로세스를 기다리면 파일이 저장되어 사용 가능하다.<br>
하지만 pipe로 출력과 입력을 연결하면 출력버퍼에서 직접 스트림을 읽을 수 있기 때문에 wait가 필요 없다.

- 파이프 관련하여<br>
    만약 `Command::spawn()`으로 프로세스를 하나 생성 할 때, 이 프로세스의 `stdin`과 `stdout`을 모두 파이프 처리했을 때 deadlock이 발생할 수 있다.<br>예를 들어
    ```rust
        use std::{
            io::{Read, Write},
            process::{Command, Stdio},
        };

        let mut ffmpeg = Command::new("ffmpeg")
            .args(&["-i", "-"])
            .args(&FFMPEG_ARGS)
            .arg("-")
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .spawn()
            .unwrap();

        let mut stdin = ffmpeg.stdin.take().unwrap();

        stdin.write_all(&data).unwrap();
        
        let mut output = vec![];
        let mut stdout = ffmpeg.stdout.take().unwrap();
        stdout.read_to_end(&mut output).unwrap();
        
        ffmpeg.wait().unwrap();
    ```
    다음과 같은 코드가 있을 때, `stdin.write_all(&data).unwrap();`부분에서 deadlock이 발생할 수 있다.<br>가능한 시나리오는 다음과 같다.<br>
    1. Rust 프로세스가 stdin.write_all(&data)을 실행함
    2. `std::io`는 `Blocking I/O`이기 때문에 Rust 프로세스는 데이터를 전부 쓰기 전까지 `write_all` 에서 블록된다.
    3. ffmpeg는 stdin에서 데이터를 소비하여 stdout에 데이터를 쓴다.
    4. ffmpeg가 작업을 진행하여 stdout가 가득 차게 되면 ffmpeg도 block된다.
    5. ffmpeg가 깨어나려면 stdout애서 데이터가 소비 되어야 한다.
    6. 그러나 ffmpeg가 Block 됐기 때문에 stdin에서 데이터를 소비할 수 없어 Rust 프로세스 또한 계속 `write_all`에서 Block 되어 있다.
    7. 따라서 실행 흐름이 `stdout.read_to_end(&mut output).unwrap();`까지 도달하지 못해 ffmpeg의 stdout을 비워 줄 수 없으므로 두 프로세스 모두 Block 되는 deadlock에 빠지게 된다.

    이를 해결하려면 
    `stdin.write_all(&data).unwrap();`를 `tokio::spawn`으로 감싸 task를 분리해주면 된다.
    
    ```rust
    tokio::spawn(async move {
        stdin.write_all(&data).await.unwrap();
    });
    ```

    이렇게 하면 `write_all`은 여전히 block되지만 다른 task에서 block되므로 기존의 task는 `stdout.read_to_end(&mut output).unwrap();`까지 도달하여 stdout을 소비하여 spawn된 프로세스를 계속 실행시킬 수 있기 때문에 deadlock이 발생하지 않게 된다.

    단, 이를 위해서 기존의 `std::process::Command`를 `tokio::process::Command`로 바꿔야한다.<br>
    `tokio::spawn`은 `Future`에 대해 동작하기 때문인데, `std::io`는 `Future`가 될 수 없기 때문이다.

    다른 방법으로는 파이프를 사용하지 않고 파일로 저장한 후 스폰 할 프로세스에서 파일을 처리하도록 하는 것도 가능하다.

---    
<br>

`Arc` 타입에 관해
- 원자적으로 동작하는 복수의 소유권을 가지는 스마트 포인터
- `Arc::clone` 하면 값을 clone하는게 아니라 소유권을 가져오면서 참조 카운팅을 늘린다.
- 단 해당 참조는 `immutable`하므로 `Mutex`나 `RwLock`를 감싸서 `Arc<Mutex<T>>`로 가변성을 얻는 식으로 많이 사용한다.

---

음성 채널 연결을 관리하는 부분을 connection handler로 분리했을 때
```rust
let manager = songbird::get(ctx)
    .await
    .expect("Songbird Voice client placed in at initialisation.");
```
이부분에서 `future cannot be sent between threads safely` 에러가 발생했다.

찾아보니 
```rust
let voice_states: &HashMap<UserId, VoiceState> = &guild_id
    .to_guild_cached(ctx)
    .unwrap()
    .voice_states;

let user_channel = voice_states
    .get(&command.user.id)
    .and_then(|voice_state| voice_state.channel_id);
```
`to_guild_cached`로 가져오는 `CacheRef`가 `NotSend`였기 때문에 `songbird::get(ctx).await` 호출 시점에도 `voice_states`가 살아있어서 발생한 문제였다.
```rust
let user_channel = {
    // CacheRef is not Send
    let voice_states: &HashMap<UserId, VoiceState> = &guild_id
        .to_guild_cached(ctx)
        .unwrap()
        .voice_states;
    voice_states
        .get(&command.user.id)
        .and_then(|voice_state| voice_state.channel_id)
}; // So must drop here
```
그래서 다음과 같이 블록을 만들어서 `voice_states`를 블록 내에서만 사용하도록 수정했다.

---
